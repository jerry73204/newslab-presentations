<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"><title>強化學習簡介</title><meta name="author" content="林祥瑞"><link rel="stylesheet" href="bower_components/reveal.js/css/reset.css"><link rel="stylesheet" href="bower_components/reveal.js/css/reveal.css"><link rel="stylesheet" href="bower_components/reveal.js/css/theme/black.css" id="theme"><!--This CSS is generated by the Asciidoctor reveal.js converter to further integrate AsciiDoc's existing semantic with reveal.js--><style type="text/css">.reveal div.right{float:right}

/* listing block */
.reveal .listingblock.stretch>.content{height: 100%}
.reveal .listingblock.stretch>.content>pre{height: 100%}
.reveal .listingblock.stretch>.content>pre>code{height:100%;max-height:100%}

/* tables */
table{border-collapse:collapse;border-spacing:0}
table{margin-bottom:1.25em;border:solid 1px #dedede}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
td.tableblock>.content{margin-bottom:1.25em}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
.reveal table th.halign-left,.reveal table td.halign-left{text-align:left}
.reveal table th.halign-right,.reveal table td.halign-right{text-align:right}
.reveal table th.halign-center,.reveal table td.halign-center{text-align:center}
.reveal table th.valign-top,.reveal table td.valign-top{vertical-align:top}
.reveal table th.valign-bottom,.reveal table td.valign-bottom{vertical-align:bottom}
.reveal table th.valign-middle,.reveal table td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{font-weight:bold}
thead{display:table-header-group}

.reveal table.grid-none th,.reveal table.grid-none td{border-bottom:0!important}

/* kbd macro */
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}

/* callouts */
.conum[data-value] {display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
/* Callout list */
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
/* Disabled from Asciidoctor CSS because it caused callout list to go under the
 * source listing when .stretch is applied (see #335)
 * .literalblock+.colist,.listingblock+.colist{margin-top:-.5em} */
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}

/* Override Asciidoctor CSS that causes issues with reveal.js features */
.reveal .hljs table{border: 0}
/* Callout list rows would have a bottom border with some reveal.js themes (see #335) */
.reveal .colist>table th, .reveal .colist>table td {border-bottom:0}
/* Fixes line height with Highlight.js source listing when linenums enabled (see #331) */
.reveal .hljs table thead tr th, .reveal .hljs table tfoot tr th, .reveal .hljs table tbody tr td, .reveal .hljs table tr td, .reveal .hljs table tfoot tr td{line-height:inherit}

/* Columns layout */
.columns .slide-content {
  display: flex;
}

.columns.wrap .slide-content {
  flex-wrap: wrap;
}

.columns.is-vcentered .slide-content {
  align-items: center;
}

.columns .slide-content > .column {
  display: block;
  flex-basis: 0;
  flex-grow: 1;
  flex-shrink: 1;
  padding: .75rem;
}

.columns .slide-content > .column.is-full {
  flex: none;
  width: 100%;
}

.columns .slide-content > .column.is-four-fifths {
  flex: none;
  width: 80%;
}

.columns .slide-content > .column.is-three-quarters {
  flex: none;
  width: 75%;
}

.columns .slide-content > .column.is-two-thirds {
  flex: none;
  width: 66.6666%;
}

.columns .slide-content > .column.is-three-fifths {
  flex: none;
  width: 60%;
}

.columns .slide-content > .column.is-half {
  flex: none;
  width: 50%;
}

.columns .slide-content > .column.is-two-fifths {
  flex: none;
  width: 40%;
}

.columns .slide-content > .column.is-one-third {
  flex: none;
  width: 33.3333%;
}

.columns .slide-content > .column.is-one-quarter {
  flex: none;
  width: 25%;
}

.columns .slide-content > .column.is-one-fifth {
  flex: none;
  width: 20%;
}
</style><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
  inlineMath: [["\\(", "\\)"]],
  displayMath: [["\\[", "\\]"]],
  ignoreClass: "nostem|nolatexmath"
},
asciimath2jax: {
  delimiters: [["\\$", "\\$"]],
  ignoreClass: "nostem|noasciimath"
},
TeX: { equationNumbers: { autoNumber: "none" } }
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.4.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script><!--Printing and PDF exports--><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "bower_components/reveal.js/css/print/pdf.css" : "bower_components/reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>強化學習簡介</h1><p class="author"><small>林祥瑞</small></p></section><section id="_參考文獻"><h2>參考文獻</h2><div class="slide-content"><div class="paragraph"><p>本文參考下列文章整理所得</p></div>
<div class="paragraph"><p><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning</a> - Lilian Weng</p></div></div></section>
<section><section id="_強化學習要解決什麼問題"><h2>強化學習要解決什麼問題？</h2><div class="slide-content"><div class="imageblock" style=""><img src="data:image/png;base64,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" alt="agent env"></div><div class="ulist"><ul><li><p>需要 <strong>玩家</strong> 和 <strong>環境</strong> 兩個基本要素</p><div class="ulist"><ul><li><p>例：圍棋、洛克人、機械手臂</p></li></ul></div></li><li><p>玩家的目標是在探索環境的過程中，將利益最大化</p></li></ul></div></div></section><section id="_公理化問題"><h2>公理化問題</h2><div class="slide-content"><div class="imageblock" style=""><img src="data:image/png;base64,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" alt="rl formulation"></div>
<div class="ulist"><ul><li><p>三要素：狀態 \(s \in \mathcal{S}\)、操作 \(a \in \mathcal{A}\)、報酬 \(r\)</p></li><li><p>玩家觀察現在的狀態 \(s\) 選擇適當的操作 \(a\)，環境根據玩家的決策 \((s, a)\) 轉移到新環境 \(s'\)，並賦予報酬 \(r\)。</p></li></ul></div></div></section><section id="_公理化問題_2"><h2>公理化問題</h2><div class="slide-content"><div class="ulist"><ul><li><p>記錄操作和狀態的序列可得 \(s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_T\)，一組 \((s_t, a_t, r_t, s_{t+1})\) 稱爲一個 episode。</p></li><li><p>強化學習演算法是這三種工具的組合：策略 \(\pi\)、模型 \(P_{s,s'}^a\)、價值或長期報酬 \(V\)</p></li></ul></div></div></section></section>
<section><section id="_環境建模"><h2>環境建模</h2><div class="slide-content"><div class="ulist"><ul><li><p>我們將環境轉移機率記爲 \(P_{s,s'}^a = \mathbb{P} [S_{t+1} = s' | S_t = s, A_t = a\)]</p></li><li><p>依賴環境建模來學習的演算法稱爲 model-based，無論這個模型已知或是學來的；反之稱爲 model-free。</p></li></ul></div></div></section><section id="_玩家策略"><h2>玩家策略</h2><div class="slide-content"><div class="ulist"><ul><li><p>策略（policy）記爲 \(\pi\)，分成兩類</p><div class="ulist"><ul><li><p>確定策略 \(a = \pi(s)\)</p></li><li><p>隨機策略 \(\pi(a|s) = \mathbb{P}[A = a | S = s\)]</p></li></ul></div></li><li><p>如果演算法會維護一個策略模型（target policy），過程中使用這策略來生成訓練樣本，稱爲 on-policy；反之，僅依賴環境的轉移的機率分佈的方法稱爲 off-policy。</p></li></ul></div></div></section><section id="_價值函數"><h2>價值函數</h2><div class="slide-content"><div class="ulist"><ul><li><p>我們希望玩家可以取得長期報酬，我們定義爲衰減報酬和，設置衰減係數是爲了避免無窮和</p></li></ul></div>
<div class="stemblock"><div class="content">\[\begin{aligned}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
= \sum_{k \ge 1} \gamma^k R_{t+k}
\end{aligned}\]</div></div>
<div class="ulist"><ul><li><p>我們可以計算這個策略 \(\pi\) 下，每個狀態的平均長期報酬，稱爲 <strong>state-value</strong></p></li></ul></div>
<div class="stemblock"><div class="content">\[V_\pi (s) = \mathbb{E}_\pi [ G_t | S_t = s, A_t = a ]\]</div></div></div></section><section id="_價值函數_2"><h2>價值函數</h2><div class="slide-content"><div class="ulist"><ul><li><p>同樣的想法，我們也可以計算特定狀態操作組合的平均長期報酬，稱爲 <strong>action-value</strong></p></li></ul></div>
<div class="stemblock"><div class="content">\[Q_\pi (s, a) = \mathbb{E}_\pi [ G_t | S_t = s, A_t = a ]\]</div></div>
<div class="ulist"><ul><li><p>我們可以組合 state-value 和 action-value 的關係</p></li></ul></div>
<div class="stemblock"><div class="content">\[V_\pi(s) = \sum_{a \in \mathcal{A}} Q_\pi (s, a) \pi(a | s)\]</div></div></div></section><section id="_演算法分類"><h2>演算法分類</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://lilianweng.github.io/lil-log/assets/images/RL_algorithm_categorization.png" alt="rl algorithm categories"></span></p></div></div></section></section>
<section><section id="_bellman_equation"><h2>Bellman Equation</h2><div class="slide-content"><div class="paragraph"><p>在 Markov 的假設下，重寫 state-value 成遞迴關係</p></div><div class="stemblock"><div class="content">\[\begin{aligned}
V(s) = \mathbb{E} [G_t | S_t = s] \\
= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t = s] \\
= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) | S_t = s ]
\end{aligned}\]</div></div><div class="paragraph"><p>同樣 action-value 也有遞迴關係</p></div><div class="stemblock"><div class="content">\[\begin{aligned}
Q(s, a) = \\
\mathbb{E} [ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})  | S_t = s, A_t = a ]
\end{aligned}\]</div></div></div></section><section id="_bellman_equation_2"><h2>Bellman Equation</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://lilianweng.github.io/lil-log/assets/images/bellman_equation.png" alt="Recursive relation"></span></p></div></div></section><section id="_特定策略_pi_下的期望值"><h2>特定策略 \(\pi\) 下的期望值</h2><div class="slide-content"><div class="stemblock"><div class="content">\[\begin{aligned}
V_{\pi}(s) &amp;= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &amp;= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &amp;= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\
Q_{\pi}(s, a) &amp;= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}\]</div></div></div></section><section id="_bellman_最佳條件"><h2>Bellman 最佳條件</h2><div class="slide-content"><div class="stemblock"><div class="content">\[\begin{aligned}
V_*(s) &amp;= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\
V_*(s) &amp;= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}\]</div></div></div></section></section>
<section id="_強化學習演算法"><h2>強化學習演算法</h2><div class="slide-content"><div class="ulist"><ul><li><p>動態規劃</p></li><li><p>蒙地卡羅法</p></li><li><p>Temporal-difference learning</p></li><li><p>Policy gradient</p></li><li><p>上述的合體</p></li></ul></div></div></section>
<section><section id="_動態規劃法"><h2>動態規劃法</h2><div class="slide-content"><div class="ulist"><ul><li><p>適合狀態空間 \(\mathcal{S}\) 和策略空間 \(\mathcal{A}\) 是離散且有限的情境</p></li><li><p>分成兩步驟：Policy Evaluation 和 Policy Improvement</p></li></ul></div></div></section><section id="_動態規劃法_2"><h2>動態規劃法</h2><div class="slide-content"><div class="ulist"><ul><li><p>Policy Evaluation：用已知的策略計算每個狀態的價值</p></li></ul></div>
<div class="stemblock"><div class="content">\[V_{t+1}(s) = \mathbb{E}_\pi [r + \gamma V_t(s) | S_t = s]\]</div></div>
<div class="ulist"><ul><li><p>Policy Improvement：用狀態價值計算更加策略</p></li></ul></div>
<div class="stemblock"><div class="content">\[Q_\pi(s, a)  = \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]\]</div></div>
<div class="ulist"><ul><li><p>將 policy evaluation 和 policy improvement 交錯執行可以逼近最佳策略 \(\pi_*\)</p></li></ul></div></div></section></section>
<section><section id="_蒙地卡羅法"><h2>蒙地卡羅法</h2><div class="slide-content"><div class="ulist"><ul><li><p>蒙地卡羅法不針對環境建模，而是對過去的經驗抽樣，去逼近期望報酬。</p></li><li><p>做法和動態規劃類似，分成 evaluation 和 improvement 兩個步驟。</p></li></ul></div></div></section><section id="_蒙地卡羅法_2"><h2>蒙地卡羅法</h2><div class="slide-content"><div class="ulist"><ul><li><p>用已知的策略 \(\pi\) 遍歷可能的狀態，並更新期望價值 \(V(s) = \frac{\sum_{t=1}^T \mathbb{1} [S_t = s] G_t}{\sum_{t=1}^T \mathbb{1} [S_t = s]}\) 、 \(Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}\)</p></li><li><p>更新策略 \(\pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)\)</p></li><li><p>重複上述的步驟，另外，遍歷時通常會搭配 ε-greedy 避免走進死胡同。</p></li></ul></div></div></section></section>
<section><section id="_temporal_difference"><h2>Temporal-Difference</h2><div class="slide-content"><div class="ulist"><ul><li><p>TD 法是一種 model-free 的演算法，不像蒙地卡羅法那樣需要遍歷所有的狀態，是現代大多數深度強化學習演算法的基礎</p></li><li><p>其想法是不斷改進現有的價值評估，稱爲 bootstrapping</p></li></ul></div></div></section><section id="_temporal_difference_2"><h2>Temporal-Difference</h2><div class="slide-content"><div class="stemblock"><div class="content">\[\begin{aligned}
V(s_t) &amp;\leftarrow (1- \alpha) V(s_t) + \alpha G_t \\
V(s_t) &amp;\leftarrow V(s_t) + \alpha (G_t - V(s_t)) \\
V(s_t) &amp;\leftarrow V(s_t) + \alpha (r_{t+1} + \gamma V(s_{t+1}) - V(s_t)) \\
Q(s_t, a_t) &amp;\leftarrow Q(s_t, a_t) + \\
&amp; \alpha (r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))
\end{aligned}\]</div></div></div></section><section id="_基於_td_的演算法"><h2>基於 TD 的演算法</h2><div class="slide-content"><div class="ulist"><ul><li><p>on-policy: SARSA</p></li><li><p>off-policy: Q-learning</p></li><li><p>兩者的做法幾乎是一樣的</p></li></ul></div></div></section><section id="_sarsa_q_learning_演算法"><h2>SARSA / Q-learning 演算法</h2><div class="slide-content"><div class="olist arabic"><ol class="arabic"><li><p>從現有的狀態 \(s_t\) 及 \(Q\) 值得推算操作 \(a_t = \arg \max_{a \in \mathcal{A}} Q (s_t, a)\)，通常會搭配 ε-greedy 使用。</p></li><li><p>進行狀態轉移 \((s_t, a_t) \rightarrow (s_{t+1}, r_{t+1})\)</p></li><li><p>再新狀態選擇操作 \(a_{t+1}\)，更新價值評估 \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \\ \alpha (r_{t+1} + \gamma  Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\)</p></li><li><p>重複上述操作</p></li></ol></div></div></section><section id="_sarsa_vs_q_learning"><h2>SARSA vs. Q-learning</h2><div class="slide-content"><div class="ulist"><ul><li><p>兩者的差別在於第三部操作 \(a_{t+1}\) 的的選擇方式</p></li><li><p>Q-learning 總是使用使用固定的方式選擇 \(a_{t+1}\)，例如 \(a_{t+1} = \arg \max_{a \in \mathcal{A}} Q(s_{t+1}, a_{t+1})\) 或搭配 ε-greedy policy</p></li><li><p>在 greedy policy 情況，兩者是一樣的 \(a_{t+1} = \arg \max_{a \in \mathcal{A}} Q(s_{t+1}, a_{t+1})\)</p></li><li><p>SARSA 則計算期望值 \(\mathbb{E}_\pi Q(s_t, a_t)\)</p></li></ul></div></div></section><section id="_sarsa_vs_q_learning_2"><h2>SARSA vs. Q-learning</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://lilianweng.github.io/lil-log/assets/images/sarsa_vs_q_learning.png" alt="SARSA vs. Q-learning"></span></p></div></div></section><section id="_deep_q_network"><h2>Deep Q-Network</h2><div class="slide-content"><div class="ulist"><ul><li><p>由於 Q-learning / SARSA 需要對狀態空間建表，在若連續狀態空間應用上有瓶頸</p></li><li><p>DeepMind 提出的解決方法：將 \(Q(s, a)\) 從表格換成深度學習網路，論文 <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">見此</a>。</p></li><li><p>更新方式改爲訓練這個損失值</p></li></ul></div>
<div class="stemblock"><div class="content">\[\mathcal{L}(\theta) = \\
\mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \big)^2 \Big]\]</div></div></div></section><section id="_deep_q_network_2"><h2>Deep Q-Network</h2><div class="slide-content"><div class="ulist"><ul><li><p>DQN 還提出兩項創新：Experience replay 、 Periodically updated target</p></li><li><p>Experience replay 是在訓練的過程中，將所有 episodes \((s_t, a_t, r_t, s_{t+1})\) 記憶下來，每完成一次迴圈後隨機取一個以前的 episode 並訓練之</p></li><li><p>Periodically updated target 是指訓練過程中維護兩個決策網路，平時使用鎖死決策網路參數 \(\pi\)，同時不斷訓練另一個決策網路 \(\pi'\)，每固定 \(C\) 步才更新一次決策網路 \(\pi \leftarrow \pi'\)</p></li></ul></div></div></section><section id="_綜合_td_及蒙地卡羅法"><h2>綜合 TD 及蒙地卡羅法</h2><div class="slide-content"><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><thead><tr><th class="tableblock halign-left valign-top">\(n\)</th><th class="tableblock halign-left valign-top">\(G_t\)</th><th class="tableblock halign-left valign-top">備註</th></tr><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">1</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">\(G_t^{(1)} = r_{t+1} + \gamma V(s_{t+1})\)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">TD learning</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">\(G_t^{(2)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 V(s_{t+2})\)</p></td><td class="tableblock halign-left valign-top"></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">\(\infty\)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">\(G_t^{(\infty)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots\)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">MC estimation</p></td></tr></table></div></section><section id="_綜合_td_及蒙地卡羅法_2"><h2>綜合 TD 及蒙地卡羅法</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png" alt="各種方式比較"></span></p></div></div></section></section>
<section><section id="_policy_gradient"><h2>Policy gradient</h2><div class="slide-content"><div class="ulist"><ul><li><p>不同於和前面提到的 DQN 等基於價值函數的模型，policy gradient 直接訓練決策函數本身</p></li><li><p>令 \(\pi_\theta\) 爲使用 \(\theta\) 參數的決策模型，我們定義這個模型的評分 \(\mathcal{J}(\theta)\)，下列公式的 \(d_{\pi_\theta}\) 爲馬可夫鏈的 stationary distribution</p></li></ul></div><div class="stemblock"><div class="content">\[\begin{aligned}
\mathcal{J}(\theta) &amp;= \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s) V_{\pi_\theta}(s) \\
&amp;= \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
\end{aligned}\]</div></div></div></section><section id="_策略函數梯度"><h2>策略函數梯度</h2><div class="slide-content"><div class="stemblock"><div class="content">\[\begin{aligned}
\nabla \mathcal{J}(\theta) &amp;= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla \pi(a \vert s; \theta) Q_\pi(s, a) \\
&amp;= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \frac{\nabla \pi(a \vert s; \theta)}{\pi(a \vert s; \theta)} Q_\pi(s, a) \\
&amp; = \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s; \theta) \nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a) \\
&amp; = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s; \theta) Q_\pi(s, a)]
\end{aligned}\]</div></div></div></section><section id="_reinforce_演算法"><h2>REINFORCE 演算法</h2><div class="slide-content"><div class="olist arabic"><ol class="arabic"><li><p>隨機初始化決策模型數 \(\theta\)</p></li><li><p>執行一次狀態轉移，假設目前已經執行 \(T\) 步，可得序列 \(s_1, a_1, r_2, s_2, a_2, \cdots, s_T\)</p></li><li><p>for t = 1, &#8230;&#8203;, T</p><div class="olist loweralpha"><ol class="loweralpha"><li><p>計算第 t 步的長期報酬 \(G_t\)</p></li><li><p>更新參數 \(\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi (a_t | s_t, \theta)\)</p></li></ol></div></li><li><p>重複 2、3 步</p></li></ol></div>
<div class="paragraph"><p>注意上述的演算法並不訓練 \(Q(s, a)\)</p></div></div></section><section id="_actor_critic_演算法"><h2>Actor-Critic 演算法</h2><div class="slide-content"><div class="paragraph"><p>Actor-critic 綜合 value-based 及 policy-based 的做法，訓練價值模型 \(Q(a|s;w)\)/\(V(s;w)\) 同時訓練決策模型 \(\pi(a|s;\theta)\)。</p></div>
<div class="paragraph"><p><span class="image"><img src="https://cdn-images-1.medium.com/max/1600/1*-GfRVLWhcuSYhG25rN0IbA.png" alt="Actor-critic"></span></p></div></div></section><section id="_actor_critic_演算法_2"><h2>Actor-Critic 演算法</h2><div class="slide-content"><div class="olist arabic"><ol class="arabic"><li><p>狀態轉移：選擇決策 \(a\) 後得新狀態 \(s'\) 及報酬 \(r_t\)，接着在新狀態 \(s'\) 選擇一個決策 \(a'\)</p></li><li><p>更新決策參數 \(\theta \leftarrow \theta + \alpha_theta \gamma^t G_t \nabla \ln \pi (a_t | s_t, \theta)\)</p></li><li><p>計算修正值 \(G_{t:t+1} = r_t + \gamma Q (s', a') - Q(s, a)\)，並更新價值模型 \(w \leftarrow w + \alpha_w G_{t:t+1} \nabla_w Q(s, a; w)\)</p></li><li><p>重複上述步驟</p></li></ol></div></div></section><section id="_a3c_演算法"><h2>A3C 演算法</h2><div class="slide-content"><div class="ulist"><ul><li><p>A3C = Asynchronous Advantage Actor-Critic</p></li><li><p>A3C 是 Actor-critic 的改良，允許多個  worker 計算參數梯度 \(\nabla \theta, \nabla w\)，每次迴圈合併每個 worker 的計算結果</p></li></ul></div></div></section></section>
<section><section id="_exploration_exploitation_難題"><h2>Exploration-Exploitation 難題</h2><div class="slide-content"><div class="ulist"><ul><li><p>Exploration-exploitation dilemma 是指演算法必須兼顧遍歷各種可能性，同時也必須強化較有前途的路徑。</p></li><li><p><a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#exploitation-vs-exploration">Multi-armed bandit</a> 便是對這問題中著名的例子</p></li></ul></div></div></section><section id="_解決_exploration_exploitation"><h2>解決 Exploration-Exploitation</h2><div class="slide-content"><div class="ulist"><ul><li><p>ε-greedy：玩家選擇策略有很小的 ε 機率選擇隨機策略，ε 可能是常數、或隨着時間遞減。</p></li><li><p>溫度法：維護一個溫度係數 \(\tau\)，越高隨機性越強，例如 \(p_i = e^{\frac{Q(s, a_i)}{1 + \tau}} / \sum_j e^{\frac{Q(s, a_j)}{1 + \tau}}\)</p></li><li><p>Upper Confidence Bounds，每個策略計算一個信賴上界 \(Q(s, a) + U\)，只要選擇這個策略的次數夠多， \(U\) 會逐漸遞減、\(Q(s, a)\) 也更精準</p></li></ul></div></div></section></section>
<section id="_q_a"><h2>Q &amp; A</h2></section></div></div><script src="bower_components/reveal.js/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
});

// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: false,
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Add the current slide number to the URL hash so that reloading the
  // page/copying the URL will return you to the same slide
  hash: false,
  // Push each slide change to the browser history. Implies `hash: true`
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // See https://github.com/hakimel/reveal.js/#navigation-mode
  navigationMode: 'default',
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Global override for preloading lazy-loaded iframes
  // - null: Iframes with data-src AND data-preload will be loaded when within
  //   the viewDistance, iframes with only data-src will be loaded when visible
  // - true: All iframes with data-src will be loaded when within the viewDistance
  // - false: All iframes with data-src will be loaded only when visible
  preloadIframes: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Specify the total time in seconds that is available to
  // present.  If this is set to a nonzero value, the pacing
  // timer will work out the time available for each slide,
  // instead of using the defaultTiming value
  totalTime: 0,
  // Specify the minimum amount of time you want to allot to
  // each slide, if using the totalTime calculation method.  If
  // the automated time allocation causes slide pacing to fall
  // below this threshold, then you will see an alert in the
  // speaker notes window
  minimumTimePerSlide: 0,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hide cursor if inactive
  hideInactiveCursor: true,
  // Time before the cursor is hidden (in ms)
  hideCursorTime: 5000,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Number of slides away from the current that are visible on mobile
  // devices. It is advisable to set this to a lower number than
  // viewDistance in order to save resources.
  mobileViewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // PDF Export Options
  // Put each fragment on a separate page
  pdfSeparateFragments: true,
  // For slides that do not fit on a page, max number of pages
  pdfMaxPagesPerSlide: 1,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'bower_components/reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'bower_components/reveal.js/plugin/notes/notes.js', async: true }
  ],

  

});</script><script>var dom = {};
dom.slides = document.querySelector('.reveal .slides');

function getRemainingHeight(element, slideElement, height) {
  height = height || 0;
  if (element) {
    var newHeight, oldHeight = element.style.height;
    // Change the .stretch element height to 0 in order find the height of all
    // the other elements
    element.style.height = '0px';
    // In Overview mode, the parent (.slide) height is set of 700px.
    // Restore it temporarily to its natural height.
    slideElement.style.height = 'auto';
    newHeight = height - slideElement.offsetHeight;
    // Restore the old height, just in case
    element.style.height = oldHeight + 'px';
    // Clear the parent (.slide) height. .removeProperty works in IE9+
    slideElement.style.removeProperty('height');
    return newHeight;
  }
  return height;
}

function layoutSlideContents(width, height) {
  // Handle sizing of elements with the 'stretch' class
  toArray(dom.slides.querySelectorAll('section .stretch')).forEach(function (element) {
    // Determine how much vertical space we can use
    var limit = 5; // hard limit
    var parent = element.parentNode;
    while (parent.nodeName !== 'SECTION' && limit > 0) {
      parent = parent.parentNode;
      limit--;
    }
    if (limit === 0) {
      // unable to find parent, aborting!
      return;
    }
    var remainingHeight = getRemainingHeight(element, parent, height);
    // Consider the aspect ratio of media elements
    if (/(img|video)/gi.test(element.nodeName)) {
      var nw = element.naturalWidth || element.videoWidth, nh = element.naturalHeight || element.videoHeight;
      var es = Math.min(width / nw, remainingHeight / nh);
      element.style.width = (nw * es) + 'px';
      element.style.height = (nh * es) + 'px';
    } else {
      element.style.width = width + 'px';
      element.style.height = remainingHeight + 'px';
    }
  });
}

function toArray(o) {
  return Array.prototype.slice.call(o);
}

Reveal.addEventListener('slidechanged', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('ready', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('resize', function () {
  layoutSlideContents(960, 700)
});</script></body></html>